{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Zewail University of Science and Technology</h1>\n",
    "<h2 align=\"center\">CIE 417 (Fall 2018)</h2>\n",
    "<h2 align=\"center\">Lab 11: Ensemble Methods</h3>\n",
    "<h3 align=\"center\">29/11/2018</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab11_functions import plot_classifier, plot_4_classifiers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods can be divided into:\n",
    "\n",
    "### 1) Averaging Methods\n",
    "\n",
    "### 2) Boosting Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#00cccc\">Averaging Methods<font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#af00af\"> Which is better, calling a friend or asking the Audiance? <font/>\n",
    "<img src=\"millionaire.jpg\">\n",
    "#### The audiance are many weak classifiers, while the friend is one strong classifier (you would generally choose someone very knowledgable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does Averaging work?\n",
    "<img src=\"Averaging1.PNG\">\n",
    "source: https://ubc-cs.github.io/cpsc340/lectures/L7.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#af00af\"> Would Averaging work better with Perceptron or Logistic Regression? <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "np.random.seed(10)\n",
    "X = np.random.randn(n,2)\n",
    "y = np.random.choice((-1,+1),size=n)\n",
    "X[y>0,0] -= 2\n",
    "X[y>0,1] += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "perceptron1 = Perceptron(random_state = 1, max_iter=1000)\n",
    "perceptron1.fit(X,y)\n",
    "perceptron2 = Perceptron(random_state = 2, max_iter=1000)\n",
    "perceptron2.fit(X,y)\n",
    "perceptron3 = Perceptron(random_state = 3, max_iter=1000)\n",
    "perceptron3.fit(X,y)\n",
    "\n",
    "perceptron_ensemble = VotingClassifier(\n",
    "    estimators=[('perceptron1', perceptron1), ('perceptron2', perceptron2), ('perceptron3', perceptron3)])\n",
    "perceptron_ensemble.fit(X,y)\n",
    "\n",
    "perceptrons = [perceptron1,perceptron2,perceptron3,perceptron_ensemble]\n",
    "\n",
    "plot_4_classifiers(X,y,perceptrons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "logistic1 = LogisticRegression(random_state=1)\n",
    "logistic1.fit(X,y)\n",
    "logistic2 = LogisticRegression(random_state=2)\n",
    "logistic2.fit(X,y)\n",
    "logistic3 = LogisticRegression(random_state=3)\n",
    "logistic3.fit(X,y)\n",
    "\n",
    "logistic_ensemble = VotingClassifier(\n",
    "    estimators=[('logistic1', logistic1), ('logistic2', logistic2), ('logistic3', logistic3)])\n",
    "logistic_ensemble.fit(X,y)\n",
    "\n",
    "logistics = [logistic1,logistic2,logistic3,logistic_ensemble]\n",
    "\n",
    "plot_4_classifiers(X,y,logistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#af00af\"> What are the conditions necessary for the averaging to increase performance? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking:\n",
    "#### Averaging can be:\n",
    "#### 1) Unweighted average (majority vote for classification)\n",
    "#### 2) Weighted average\n",
    "### $$ y_i = f_i(x_1,x_2,...) $$\n",
    "### $$ y_e = \\sum_{i} \\alpha_i y_i\\ \\ \\ \\ (regression)$$\n",
    "### $$ y_e = sign(\\sum_{i} \\alpha_i y_i)\\ \\ \\ \\ (classification)$$\n",
    "#### 3) Stacking (predictor of predictors)\n",
    "### $$ y_i = f_i(x_1,x_2,...) $$\n",
    "### $$ y_e = f_e(y_1,y_2,..)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#af00af\"> If $f_e$ is linear, what does that mean? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#af00af\"> Can we use averaging with Linear Regression? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#af00af\"> how can we train the weighted average weights or the stacking predictor parameters? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging (Bootstrap Aggregation)\n",
    "### It helps add randomness to decrease correlation and increase independence between ensemble classifiers\n",
    "## Bootstrapping:\n",
    "### Given a dataset of size N, sample a new dataset of size N' \"with replacement\"\n",
    "## Bagging:\n",
    "### 1) Create several Bootstrap samples\n",
    "### 2) Fit a classifier to each sample\n",
    "### 3) Average the classifiers predictions in test time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#af00af\"> What are the benefits of bagging? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "### Random Forests are bagging applied on decision \"trees\" (hence the name forests)\n",
    "### Random Forests increase randomness even more by choosing the optimal feature for each node from a subset of \"randomly chosen\" features\n",
    "<img src=\"RF.jpg\">\n",
    "source: https://www.youtube.com/watch?v=-bYrLRMT3vY\n",
    "### <font color = \"#cc2f2f\"> Random Forests are one of the strongest models available, they decrease overfitting through averaging many high variance decision trees, and decrease computational power needed by choosing only a subset of the features each time (hence they are suitable for problems with very large amount of features) <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#af00af\"> Why are decision trees the perfect candidate for bagging? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#af00af\"> How can Random Forests be considered feature selectors? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Random Forests Hyperparameters:\n",
    "### 1) Size of bootstrap samples\n",
    "### 2) Number of random features chosen each time (the lower this number, the more uncorrelated the trees become)\n",
    "### 3) Forest size (number of trees)\n",
    "<img src=\"RFSize.png\">\n",
    "source: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/decisionForests_MSR_TR_2011_114.pdf\n",
    "### 4) Maximum allowed tree depth\n",
    "<img src=\"RFDepth.png\">\n",
    "source: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/decisionForests_MSR_TR_2011_114.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Bagging:\n",
    "### <font color = \"#cc2f2f\"> Random Forests without bagging act as a Max-Margin classifier\n",
    "<img src=\"Bagging.png\">\n",
    "source: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/decisionForests_MSR_TR_2011_114.pdf\n",
    "### Decision Tree trained on full dataset\n",
    "<img src=\"Full_DT.PNG\">\n",
    "source: http://sli.ics.uci.edu/Classes/2016W-178?action=download&upname=10_ensembles.pdf\n",
    "### Decision Trees trained on bootstrap samples (bagging) and averaged\n",
    "<img src=\"Avg_DTs.PNG\">\n",
    "source: http://sli.ics.uci.edu/Classes/2016W-178?action=download&upname=10_ensembles.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#af00af\"> Can Random Forest weak classifiers be trained in parallel? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#afaf00\"> Boosting (Adaboost) <font/>\n",
    "### Train classifiers sequentially with each classifier focusing on what the previous classifiers got wrong\n",
    "### The resulting classifier is a combination of all the weak classifiers (as in weighted averaging)\n",
    "### $$ y_e = sign(\\sum_{t} \\alpha_t y_t)$$\n",
    "### $$ \\alpha_t = \\frac{1}{2} \\ln(\\frac{1-\\epsilon_t}{\\epsilon_t})$$\n",
    "### $$ \\epsilon_t =\\ error\\ rate\\ for\\ the\\ t^{th}\\ classifier\\ = \\frac{number\\ of\\ misclassifications\\ for\\ the\\ t^{th}\\ classifier}{training\\ set\\ size}$$\n",
    "<img src=\"adaboost_alphacurve.png\">\n",
    "source: http://mccormickml.com/2013/12/13/adaboost-tutorial/\n",
    "### <font color = \"#cc2f2f\"> Stronger classifier has a lower error rate, hence a higher weight (with weight of zero for 50% classifiers)\n",
    "### $$ D_{t+1}(i) = \\frac{D_t(i)e^{-\\alpha_tt_iy_t(i)}}{Z_t}$$\n",
    "### Where $D_t(i)$ is the weight of the $i^{th}$ point for the $t^{th}$ classifier, $Z_t$ is a normalization constant\n",
    "<img src=\"exp_x.png\">\n",
    "source: http://mccormickml.com/2013/12/13/adaboost-tutorial/\n",
    "### <font color = \"#cc2f2f\"> Points which are misclassified ($t_i$ is positive and $y_t(i)$ is negative or vice versa) have a positive exponential term, and hence their weights increase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#ff0000\"> Exercise: <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lab11_functions import plot_iris_data\n",
    "\n",
    "# import iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# We would use only the first two features\n",
    "X = iris.data[:,:2]\n",
    "y = iris.target\n",
    "\n",
    "#split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, shuffle = True, random_state = 0)\n",
    "\n",
    "del X, y\n",
    "print (f\"training set size: {X_train.shape[0]} samples \\ntest set size: {X_test.shape[0]} samples\")\n",
    "plot_iris_data(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#ff0000\"> Train a Decision Tree model using Information Gain criterion, print training and testing accuracies <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#ff0000\"> Train a Random Forest model using Information Gain criterion, maximum depth of 3, 10 classifiers, and random_state=3, print training and testing accuracies <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#ff0000\"> Train an adaboost ensemble model using decision trees classifier with Information Gain criterion, maximum depth of 3, 10 classifiers, and random_state=3, print training and testing accuracies <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
