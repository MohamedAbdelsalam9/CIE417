{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Zewail University of Science and Technology</h1>\n",
    "<h2 align=\"center\">CIE 417 (Fall 2018)</h2>\n",
    "<h2 align=\"center\">Lab 9: PCA</h3>\n",
    "<h3 align=\"center\">15/11/2018</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#00cccc\">PCA<font/>\n",
    "### Run PCA Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#af00af\"> What does PCA do? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) PCA learns a k-dimensional subspace given a d-dimensional data\n",
    "### 2) The new subspace is represented by k basis vectors\n",
    "### 3) The k basis vectors are orthonormal, and they try to capture as much variance in the original data as possible\n",
    "### 4) The 1st basis vector (1st principal component) points in the direction of the data with maximum variance, and so forth\n",
    "### $$ X \\approx ZW $$\n",
    "### $$ (nxd) \\approx (nxk)(kxd) $$\n",
    "\n",
    "### The basis vectors are the rows of W\n",
    "### The representations in the new basis are the rows of Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"PCA_Linear_Combination.PNG\">\n",
    "source: https://ubc-cs.github.io/cpsc340/lectures/L24.pdf\n",
    "### Instead of using 784-dimensional images (28x28), we can use the 7-dimensional z to represent them (for \"3\", the z = [1,1,1,1,1,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#af00af\"> How to calculate principal components? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Subtract the mean of the data (and preferably standardize the data)\n",
    "### 2) Calculate the covariance matrix\n",
    "### $$ \\Sigma = \\frac{1}{n-1}(X-\\mu)^T(X-\\mu) = \\frac{1}{n-1}X^T X\\ (as\\ we\\ centered\\ the\\ data)$$ \n",
    "### 3) Calculate the eigenvalues and the eigenvectors of the covariance matrix\n",
    "### 4) Construct the transformation matrix W, with the rows being the eigenvectors that correspond to the k biggest eigenvalues, these eigenvectors represent your new basis vectors\n",
    "\n",
    "### Alternatively, PCA can be calculated using SVD (which is more common)\n",
    "### See here for more details on SVD: https://www.youtube.com/watch?v=daHVmoOrLrI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IRIS Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attributes:\n",
    "1. sepal length in cm \n",
    "2. sepal width in cm \n",
    "3. petal length in cm \n",
    "4. petal width in cm \n",
    "\n",
    "We will just use two features for easier visualization; sepal length and width.\n",
    "\n",
    "##### class: \n",
    "* Iris Setosa \n",
    "* Iris Versicolour \n",
    "* Iris Virginica\n",
    "\n",
    "<img src=\"Lab9_petal_sepal.png\">\n",
    "source: https://www.wpclipart.com/plants/diagrams/plant_parts/petal_sepal_label.png.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# We would use only the first two features\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "#split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, shuffle = True, random_state = 0)\n",
    "\n",
    "del X, y\n",
    "print (f\"training set size: {X_train.shape[0]} samples \\ntest set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import svd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mu = np.mean(X_train,axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "X_train_std = (X_train-mu)/std\n",
    "del X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#af00af\"> Why do we need to standardize data? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PCA to Reduce Features Dimensions From 4 to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "new_projected_data = pca.fit_transform(X_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab9_functions import plot_iris_data\n",
    "plot_iris_data(new_projected_data,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#af00af\"> Which features do these two components represent? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#af00af\"> How much information did we lose in the previous operation? <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#af00af\"> What are the applications of PCA? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning (reduce features size for computational purposes, avoid overfitting, etc)\n",
    "### Visualization\n",
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#af00af\"> When does PCA fails? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"PCA_fails.png\">\n",
    "source: A Tutorial on Principal Component Analysis, Jonathon Shlens, https://arxiv.org/pdf/1404.1100.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#ff0000\"> Exercise: <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X,y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(X[0,:].reshape(8,8), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data to Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, shuffle = True, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Center Data (no need to divide by standard deviation as they are all on the same scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Sklearn LinearSVC classifier to classify data, and print the training and testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply PCA using two components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data (with different colors for different numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Sklearn LinearSVC classifier to classify the new projected data, and print the training and testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply PCA such that 90% of the variance in the data is reserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many components did you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Sklearn LinearSVC classifier to classify the new projected data, and print the training and testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
