{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Zewail University of Science and Technology</h1>\n",
    "<h2 align=\"center\">CIE 417 (Fall 2018)</h2>\n",
    "<h2 align=\"center\">Lab 8: K-means and GMM</h3>\n",
    "<h3 align=\"center\">08/11/2018</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#af00af\"> What is the difference between supervised and unsupervised learning? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#00cccc\">K-means<font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Demo sources: \n",
    "\n",
    "https://github.com/UBC-CS/cpsc340/blob/master/lectures/L8demo.ipynb \n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make up some fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from lab8_functions import generate_data\n",
    "from lab8_functions import plot_clust\n",
    "from lab8_functions import plot_gmm\n",
    "\n",
    "\n",
    "X, z_true = generate_data(data_type = 1, random_state = 170, n_samples = 300, k = 4)\n",
    "n = len(X)\n",
    "plot_clust(X,z=z_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### randomly initialize means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "npr.seed(2)\n",
    "\n",
    "W = X[npr.choice(n, k, replace=False)]\n",
    "\n",
    "plot_clust(X,W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assign each object to closest mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_z(X,W):\n",
    "    dist2 = euclidean_distances(X, W)\n",
    "    return np.argmin(dist2, axis=1)\n",
    "\n",
    "z = update_z(X,W)\n",
    "plot_clust(X,W,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recompute cluster centres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_W(X,z,W_old):\n",
    "    # just being a bit careful about the case of a cluster with no points in it\n",
    "    W = W_old.copy()\n",
    "    for kk in range(k):\n",
    "        W[kk] = np.mean(X[z==kk],axis=0)\n",
    "    return W\n",
    "\n",
    "W = update_W(X,z,W)\n",
    "plot_clust(X,W,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run the whole thing for 100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better: loop until np.all(z_new == z)\n",
    "for itr in range(100):\n",
    "    z = update_z(X,W)\n",
    "    W = update_W(X,z,W)\n",
    "plot_clust(X,W,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#af00af\"> Can we use accuracy to measure performance? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#af00af\"> What is the Kmeans convergence criteria? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#af00af\"> Can we guarantee convergence in Kmeans? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#af00af\"> What are the problems of kmeans? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "npr.seed(3)\n",
    "\n",
    "X, z_true = generate_data(data_type = 1, random_state = 170, n_samples = 300, k = 4)\n",
    "n = len(X)\n",
    "k = 4\n",
    "W = X[npr.choice(n, k, replace=False)]\n",
    "plot_clust(X,W)\n",
    "\n",
    "for itr in range(100):\n",
    "    z = update_z(X,W)\n",
    "    W = update_W(X,z,W)\n",
    "    \n",
    "plot_clust(X,W,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Knowing k beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "npr.seed(2)\n",
    "\n",
    "X, z_true = generate_data(data_type = 1, random_state = 170, n_samples = 300, k = 4)\n",
    "n = len(X)\n",
    "k = 3\n",
    "W = X[npr.choice(n, k, replace=False)]\n",
    "plot_clust(X,W)\n",
    "\n",
    "for itr in range(100):\n",
    "    z = update_z(X,W)\n",
    "    W = update_W(X,z,W)\n",
    "    \n",
    "plot_clust(X,W,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Assuming Isotropic Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "npr.seed(1)\n",
    "\n",
    "X, z_true = generate_data(data_type = 2, random_state = 170, n_samples = 300, k = 3)\n",
    "n = len(X)\n",
    "k = 3\n",
    "W = X[npr.choice(n, k, replace=False)]\n",
    "plot_clust(X,W)\n",
    "\n",
    "for itr in range(100):\n",
    "    z = update_z(X,W)\n",
    "    W = update_W(X,z,W)\n",
    "    \n",
    "plot_clust(X,W,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Assuming Equal Variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "npr.seed(1)\n",
    "\n",
    "X, z_true = generate_data(data_type = 3, random_state = 170, n_samples = 400, k = 3)\n",
    "n = len(X)\n",
    "k = 3\n",
    "W = X[npr.choice(n, k, replace=False)]\n",
    "plot_clust(X,W)\n",
    "\n",
    "for itr in range(100):\n",
    "    z = update_z(X,W)\n",
    "    W = update_W(X,z,W)\n",
    "    \n",
    "plot_clust(X,W,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Assuming Convex Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lab8_plot_kmeans_digits_001.png\">\n",
    "source: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "npr.seed(1)\n",
    "\n",
    "X, z_true = generate_data(data_type = 4, random_state = 170, n_samples = 400, k = 2)\n",
    "n = len(X)\n",
    "k = 2\n",
    "W = X[npr.choice(n, k, replace=False)]\n",
    "plot_clust(X,W)\n",
    "\n",
    "for itr in range(100):\n",
    "    z = update_z(X,W)\n",
    "    W = update_W(X,z,W)\n",
    "    \n",
    "plot_clust(X,W,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#cc0000\"> Gaussian Mixture Models (GMM)<font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nice video with easy explanation: https://www.youtube.com/watch?v=JNlEIEwe-Cg\n",
    "<img src=\"lab8_GMM_1_2d.PNG\">\n",
    "source: https://github.com/llSourcell/Gaussian_Mixture_Models/blob/master/intro_to_gmm_%26_em.ipynb\n",
    "<img src=\"lab8_GMM.png\">\n",
    "source: https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization Algorithm\n",
    "### Expectation (E) step: Given the current parameters of the model, estimate a probability distribution.\n",
    "<img src=\"lab8_expectation.png\" height=\"600\" width=\"600\">\n",
    "\n",
    "### Maximization (M) step: Given the current data, estimate the parameters to update the model.\n",
    "<img src=\"lab8_maximization.png\" height=\"600\" width=\"600\">\n",
    "source: https://www.youtube.com/watch?v=qMTuMa86NzU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#af00af\"> What is the relation between GMM and Kmeans? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#af00af\"> Can we solve the shortcomings of kmeans with GMM? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Knowing k beforehand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture \n",
    "\n",
    "X, z_true = generate_data(data_type = 1, random_state = 170, n_samples = 300, k = 4)\n",
    "gmm = GaussianMixture(n_components=3)\n",
    "plot_gmm(gmm, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Assuming Isotropic Clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture \n",
    "\n",
    "X, z_true = generate_data(data_type = 2, random_state = 170, n_samples = 300, k = 3)\n",
    "gmm = GaussianMixture(n_components=3)\n",
    "plot_gmm(gmm, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Assuming Equal Variances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture \n",
    "\n",
    "X, z_true = generate_data(data_type = 3, random_state = 170, n_samples = 400, k = 3)\n",
    "gmm = GaussianMixture(n_components=3)\n",
    "plot_gmm(gmm, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Assuming Convex Clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture \n",
    "\n",
    "X, z_true = generate_data(data_type = 4, random_state = 170, n_samples = 400, k = 2)\n",
    "gmm = GaussianMixture(n_components=2)\n",
    "plot_gmm(gmm, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#ff0000\"> Exercise <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, z_true = generate_data(data_type = 2, random_state = 170, n_samples = 400, k = 2)\n",
    "plot_clust(X, z=z_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = \"#ff0000\"> Using Sklearn, get the Kmeans clusters of X (and put them in z) <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### \n",
    "## write your code here \n",
    "####\n",
    "\n",
    "plot_clust(X=X,z=z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = \"#ff0000\"> Using Sklearn, get the GMM clusters of X (and put them in z) <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### \n",
    "## write your code here \n",
    "####\n",
    "\n",
    "plot_clust(X=X,z=z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = \"#ff0000\"> By changing the parameters of the GMM, make it reproduce the same results as the kmeans (and why does that happen?) <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### \n",
    "## write your code here \n",
    "####\n",
    "\n",
    "plot_clust(X=X,z=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
