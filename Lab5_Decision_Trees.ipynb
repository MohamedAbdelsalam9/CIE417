{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Zewail University of Science and Technology</h1>\n",
    "<h2 align=\"center\">CIE 417 (Fall 2018)</h2>\n",
    "<h2 align=\"center\">Lab 5: Cross Validation and Decision Trees</h3>\n",
    "<h3 align=\"center\">18/10/2018</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#b1b100\">Cross Validation <font/>\n",
    "<a href=\"https://dziganto.github.io/cross-validation/data%20science/machine%20learning/model%20tuning/python/Model-Tuning-with-Validation-and-Cross-Validation/\">This link<a/> <font color = \"#000000\">provides a good tutorial on bias variance tradeoff and validation/cross-validation<font/>\n",
    "\n",
    "<img src=\"bias-variance-tradeoff.png\">\n",
    "source: https://dziganto.github.io/cross-validation/data%20science/machine%20learning/model%20tuning/python/Model-Tuning-with-Validation-and-Cross-Validation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#af00af\"> 1) How can we tune hyperparameters if the dataset is small that we can't afford creating a separate validation set? <font/>\n",
    "<img src=\"cross_validation.gif\">\n",
    "source: https://imada.sdu.dk/~marco/Teaching/AY2010-2011/DM825/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross validation (3-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "kf = KFold(n_splits=3)\n",
    "print (\"Original Data: \", X)\n",
    "print (\"Number of Splits: \", kf.get_n_splits(X))\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", X[train_index], \"TEST:\", X[test_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave One Out (LOO) cross validation (n-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "print (\"Original Data: \", X)\n",
    "loo = LeaveOneOut()\n",
    "print (\"Number of Splits: \", loo.get_n_splits(X))\n",
    "for train_index, test_index in loo.split(X):\n",
    "    print(\"TRAIN:\", X[train_index], \"TEST:\", X[test_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave P Out (LPO) cross validation (Leave 2 Out) \n",
    "Creates overlapping folds, unlike kfold method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeavePOut\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "print (\"Original Data: \", X)\n",
    "loo = LeavePOut(p = 2)\n",
    "print (\"Number of Splits: \", loo.get_n_splits(X))\n",
    "for train_index, test_index in loo.split(X):\n",
    "    print(\"TRAIN:\", X[train_index], \"TEST:\", X[test_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules of thumb: \n",
    "1. generally, use 5-fold or 10-fold cross-validation\n",
    "2. avoid LOO and LPO for large datasets, as they are very costly\n",
    "3. as the dataset gets larger, decrease the number of folds (3-fold, 2-fold, etc)\n",
    "4. if the dataset is very large, avoid cross validation altogether and just dedicate a separate validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Boston dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attributes:\n",
    "1. crim: \n",
    "per capita crime rate by town.\n",
    "\n",
    "2. zn: \n",
    "proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "3. indus: \n",
    "proportion of non-retail business acres per town.\n",
    "\n",
    "4. chas: \n",
    "Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "\n",
    "5. nox: \n",
    "nitrogen oxides concentration (parts per 10 million).\n",
    "\n",
    "6. rm: \n",
    "average number of rooms per dwelling.\n",
    "\n",
    "7. age: \n",
    "proportion of owner-occupied units built prior to 1940.\n",
    "\n",
    "8. dis: \n",
    "weighted mean of distances to five Boston employment centres.\n",
    "\n",
    "9. rad: \n",
    "index of accessibility to radial highways.\n",
    "\n",
    "10. tax: \n",
    "full-value property-tax rate per \\$10,000.\n",
    "\n",
    "11. ptratio: \n",
    "pupil-teacher ratio by town.\n",
    "\n",
    "12. black: \n",
    "1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n",
    "\n",
    "13. lstat: \n",
    "lower status of the population (percent).\n",
    "\n",
    "##### Target:\n",
    "median value of owner-occupied homes in \\$1000s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import Boston dataset\n",
    "boston = datasets.load_boston()\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "#split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True, random_state = 0)\n",
    "\n",
    "del X, y\n",
    "print (f\"training set size: {X_train.shape[0]} samples \\ntest set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Standardize Dataset (substract training set mean and divide by training set standard deviation)\n",
    "scaler = StandardScaler().fit(X_train) #the scaler is fitted to the training set (it gets the mean and std of the training set)\n",
    "X_train_standardized = scaler.transform(X_train) #the scaler is applied to the training set\n",
    "X_test_standardized = scaler.transform(X_test) #the scaler is applied to the test set\n",
    "\n",
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#ef0000\"> Exercise 1: Train using scikit-learn Ridge, Use 5-fold cross-validation for the choice of the regularization parameter (alpha) using scikit-learn cross_validate) <font/>\n",
    "#### <font color=\"#ef0000\"> Write your code here <font/>\n",
    "<font color=\"#ef0000\"> use neg_mean_squared_error as the scoring parameter, print the training error and the cross validation error for each alpha <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "alphas = [1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3]\n",
    "k = 5\n",
    "\n",
    "## write your code here, print the training error and the cross validation error for each alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#a15151\">Decision Trees <font/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"restaurant-tree.png\">\n",
    "<br/> \n",
    "<br/> \n",
    "source: Artificial Intelligence A Modern Approach by Stuart Russell and Peter Norvig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#af00af\"> What if there is a continuous variable? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#af00af\"> How can we obtain a continuous output (Regression Tree)? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"regression_tree.png\">\n",
    "\n",
    "source: Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani and Jerome Friedman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#af00af\"> Which is the better choice for a node, and why? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"restaurant-stub.png\">\n",
    "<br/> \n",
    "<br/> \n",
    "source: Artificial Intelligence A Modern Approach by Stuart Russell and Peter Norvig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#af00af\"> Why don't we try all tree combinations and find the optimum tree? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "#### Entropy represents the amount of randomness, the more we know (have information), the less the entropy becomes\n",
    "\n",
    "<img src=\"EntropyVersusProbability.png\">\n",
    "\n",
    "source: http://matlabdatamining.blogspot.com/2006/11/introduction-to-entropy.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We recursively choose the node that reduces the entropy (the node with the maximum Information Gain)\n",
    "\n",
    "$$Entropy\\ H(\\pi) = -\\sum \\pi log_2(\\pi)$$\n",
    "\n",
    "$$Two\\ Class\\ Entropy\\ H(\\frac{p}{p+n},\\frac{n}{p+n}) = -\\frac{p}{p+n} log_2(\\frac{p}{p+n}) -\\frac{n}{p+n} log_2(\\frac{n}{p+n})$$\n",
    "\n",
    "$$Expected\\ Entropy\\ after\\ adding\\ node\\ A\\ EH(A)=\\sum_{i=1}^k probability\\ of\\ leaf\\ i\\ X\\ entropy\\ of\\ leaf\\ i$$\n",
    "\n",
    "$$EH(A)= \\sum_{i=1}^k \\frac{p_i+n_i}{p+n} H(\\frac{p_i}{p+n},\\frac{n_i}{p+n}) $$\n",
    "\n",
    "$$Information\\ Gain\\ I(A)= H(\\frac{p}{p+n},\\frac{n}{p+n}) - EH(A)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#af00af\"> Does this greedy algorithm guarantees getting the optimum tree? <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#af00af\"> When to stop creating more nodes? <font/>\n",
    "* #### If all the remaining examples are all positive or negative\n",
    "* #### If there are no examples left for this certain case (return the dominant class of the parent node)\n",
    "* #### If there are no more attributes (return the dominant class of the remaining examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#ef0000\"> Exercise 2: Determine (by hand) the first 2 nodes of a decision tree for this data<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patrons</th>\n",
       "      <th>Type</th>\n",
       "      <th>Hungry</th>\n",
       "      <th>WillWait (y)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Some</td>\n",
       "      <td>French</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Full</td>\n",
       "      <td>Thai</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Some</td>\n",
       "      <td>Burger</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Full</td>\n",
       "      <td>Thai</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Full</td>\n",
       "      <td>French</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Some</td>\n",
       "      <td>Italian</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "      <td>Burger</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Some</td>\n",
       "      <td>Thai</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Full</td>\n",
       "      <td>Burger</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Full</td>\n",
       "      <td>Italian</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>None</td>\n",
       "      <td>Thai</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Full</td>\n",
       "      <td>Burger</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Patrons     Type  Hungry  WillWait (y)\n",
       "0     Some   French    True          True\n",
       "1     Full     Thai    True         False\n",
       "2     Some   Burger   False          True\n",
       "3     Full     Thai    True          True\n",
       "4     Full   French   False         False\n",
       "5     Some  Italian    True          True\n",
       "6     None   Burger   False         False\n",
       "7     Some     Thai    True          True\n",
       "8     Full   Burger   False         False\n",
       "9     Full  Italian    True         False\n",
       "10    None     Thai   False         False\n",
       "11    Full   Burger    True          True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Patrons = [\"Some\", \"Full\", \"Some\", \"Full\", \"Full\", \"Some\", \"None\", \"Some\", \"Full\", \"Full\", \"None\", \"Full\"]\n",
    "Type = [\"French\", \"Thai\", \"Burger\", \"Thai\", \"French\", \"Italian\", \"Burger\", \"Thai\", \"Burger\", \"Italian\", \"Thai\", \"Burger\"]\n",
    "Hungry = [True, True, False, True, False, True, False, True, False, True, False, True]\n",
    "WillWait = [True, False, True, True, False, True, False, True, False, False, False, True]\n",
    "df2 = pd.DataFrame({'Patrons': Patrons, \"Type\": Type, \"Hungry\": Hungry, \"WillWait (y)\": WillWait})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#ef0000\"> Exercise 3: Classify the iris dataset using scikit-learn DecisionTreeClassifier <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# We would use only the first two features\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "#split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, shuffle = True, random_state = 1)\n",
    "\n",
    "del X, y\n",
    "print (f\"training set size: {X_train.shape[0]} samples\\ntest set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"#ef0000\"> Write Your Code Here <font/>\n",
    "<font color=\"#ef0000\"> use entropy as the criterion parameter, print the training and testing accuracy <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "## write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#ef0000\"> Exercise 4: Predict the Boston dataset used in Exercise 1 using scikit-learn DecisionTreeRegressor <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import Boston dataset\n",
    "boston = datasets.load_boston()\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "#split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True, random_state = 0)\n",
    "\n",
    "del X, y\n",
    "print (f\"training set size: {X_train.shape[0]} samples \\ntest set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"#ef0000\"> Write Your Code Here <font/>\n",
    "<font color=\"#ef0000\"> use entropy as the criterion parameter, print the training and testing accuracy <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "## write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#af00af\"> Advantages of Decision Trees? <font/>\n",
    "* #### Interpretability of Results\n",
    "* #### Powerful nonparametric model\n",
    "* #### It has no problem whether the inputs and outputs are binary, categorical or continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"#af00af\"> Problems of Decision Trees? <font/>\n",
    "* #### Overfitting\n",
    "* #### Very High Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#ff7777\"> Try to solve Assignment 2 before next midterm (It will be included in the exam) <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
